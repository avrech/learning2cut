# exp4:
# train on random graphs, generate demonstrations and cut_selection_dqn data, use only demonstration loss.

# NOTES:
# All intervals are measured in num_param_updates,
# except target_update_interval and param_update_interval which are measured in number of sgd steps done on the learner.
# SINGLE THREAD SETTING:
# - num_policy_updates equals to the number of episodes done and to the number of sgd steps done.
# - it is recommended to sparsify the logs and checkpoints to reduce the tensorboard memory consumption.
# - specifically it is recommended to evaluate the hard validation and test sets in low frequency.
# DISTRIBUTED SETTING:
# - num_policy_updates equals to the number of workers' policy updates, which happen every param_update_interval
# - the hard datasets can be evaluated on the tester in the same frequency as the easy datasets.
# DQN ALGORITHM HPARAMS
batch_size: 32 # 128
lr: 0.001
weight_decay: 0.0001
nstep_learning: 1 # 1, 8
target_update_interval: 1000  # num sgd steps between target_net's parameters update
value_aggr: mean  # tqnet v2 works only with max. the other models work also with mean
credit_assignment: True  # False, True
gamma: 0.99
eps_start: 0.9
eps_end: 0.05
eps_decay: 50000 #100000
dqn_objective: db_auc  # options: db_auc, gap_auc
empty_action_penalty: 0
select_at_least_one_cut: True
update_rule: DQN
discard_bad_experience: False  # discard training episodes which terminated before LP_ITERATIONS_LIMIT

# LEARNING FROM DEMONSTRATIONS
n_step_loss_coef: 1.0
demonstration_loss_coef: 0.5 # 1.0
demonstration_large_margin: 0.05 # 1

# DQN ARCHITECTURE HPARAMS
dqn_arch: SCIPTuningQNet
emb_dim: 64
cut_conv: CutConv   # options: CutConv, CATConv
encoder_cut_conv_layers: 1
encoder_lp_conv_layers: 1
decoder_layers: 1
decoder_conv: RemainingCutsConv
attention_heads: 4
lp_conv_aggr: mean
cut_conv_aggr: mean

# GENERAL
scip_env: tuning
action_set:
  objparalfac: [0.1, 0.5, 1]
  dircutoffdistfac: [0.1, 0.5, 1]
  efficacyfac: [0.1, 0.5, 1]
  intsupportfac: [0.1, 0.5, 1]
  maxcutsroot: [5, 15, 2000]
  minorthoroot: [0.5, 0.9, 1]
seed: 259385
hide_scip_output: True
num_episodes: 2000000
log_interval: 5
checkpoint_interval: 5
verbose: 1
ignore_test_early_stop: True  # report performance ignoring early stops due to branching
aggressive_separation: True
slack_tol: 0.000001

# DEBUG FLAGS
debug: False
debug_cuda: False
overfit: False
sanity_check: False  # True - todo - make it working with demonstrations. currently it doesn't.
fix_training_scip_seed: 223  # 0 -> use random seed, 0 < int -> fix the specified seed. 223 is scip seed used in solving trainset for default baselines. useful for comparing agent on trainset.

# SCIP STUFF
#reset_maxcuts_every_round: True  # avoid undesired "unoughcuts" in SCIPseparationRoundLP
use_general_cuts: True
reset_maxcuts: 100
reset_maxcutsroot: 100

# CYCLE INEQUALITIES
chordless_only: False  #
simple_cycle_only: True
enable_chordality_check: False  # set True to count how many chordless cycles applied, and to filter chordal cycles. WARNING! time consuming!
record_cycles: False  # set True to record the generated cycles along test episodes.

# DISTRIBUTED RL PARAMETERS

# Learning parameters
learner_gpu: True
worker_gpu: False
#tester_gpu: False
num_workers: 3
num_learners: 1
param_sync_interval: 1 # 100  # number of learner.sgd_steps between workers policy update
max_num_updates: 100000  # relevant to PER to increase beta across time

# Buffer parameters
replay_buffer_capacity: 40000
replay_buffer_n_demonstrations: 0 #10k
replay_buffer_demonstration_priority_bonus: 0.0001  # todo - tune
replay_buffer_minimum_size: 4000 #10k
replay_buffer_max_mem: 40
use_per: True
priority_alpha: 0.6
priority_beta: 0.4
priority_beta_start: 0.4
priority_beta_end: 1.0
priority_beta_decay: 100000  # exponentially increase to priority_beta_end like decreasing eps_greedy
local_buffer_size: 500  #500 todo tune this parameter on the real distributed program
