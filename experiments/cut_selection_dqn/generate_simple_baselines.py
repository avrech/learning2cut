"""
Loads the datasets generated by generate_dataset.py
and solves each instance with 2 simple baselines:
1. forcing 10 random cuts
2. forcing 10 most violated cuts
Stores those baselines in dataset as '10_random' and '10_most_violated'.
"""

from utils.scip_models import maxcut_mccormic_model, MccormickCycleSeparator
import pickle
import os
import numpy as np
import networkx as nx
from tqdm import tqdm
from ray import tune
from copy import deepcopy


def solve_graphs(worker_config):
    """
    Worker thread. Solves graphs assigned to worker according to the specifications in config.
    """
    nworkers = worker_config['nworkers']
    workerid = worker_config['workerid']
    datadir = worker_config['datadir']
    quiet = worker_config.get('quiet', False)
    for dataset_config in worker_config['datasets'].values():

        assert dataset_config.get('save_all_stats', False)

        ngraphs = dataset_config['ngraphs']

        if ngraphs >= nworkers:
            if workerid == nworkers - 1:
                # so assign to the last worker what left to complete ngraphs
                worker_ngraphs = int(ngraphs - (nworkers - 1) * np.floor(ngraphs / nworkers))
            else:
                worker_ngraphs = int(np.floor(ngraphs / nworkers))
        else:
            # assign 1 graph to each one of the first ngraphs workers, and terminate the other threads
            if workerid < ngraphs:
                worker_ngraphs = 1
            else:
                # there is no work left to do.
                continue

        nmin, nmax = dataset_config["graph_size"]['min'], dataset_config["graph_size"]['max']
        m = dataset_config["barabasi_albert_m"]
        weights = dataset_config["weights"]
        seed = dataset_config["seed"]
        np.random.seed(seed)
        dataset_dir = os.path.join(datadir,
                                   dataset_config['dataset_name'],
                                   f"barabasi-albert-nmin{nmin}-nmax{nmax}-m{m}-weights-{weights}-seed{seed}")

        for graph_idx in tqdm(range(worker_ngraphs), desc=f'Worker {workerid}: solving graphs...'):
            filepath = os.path.join(dataset_dir, f"graph_{workerid}_{graph_idx}.pkl")
            with open(filepath, 'rb') as f:
                G, baseline = pickle.load(f)
                assert baseline is not None

            sepa_hparams = {
                'max_per_root': 1000000,
                'max_per_node': 1000000,
                'max_per_round': 10,  # todo control parameter via args
                'forcecut': True,
                'cuts_budget': 1000000,
                'max_cuts_applied_node': 1000000,
                'max_cuts_applied_root': 1000000,
                'record': True,
                'lp_iterations_limit': dataset_config['lp_iterations_limit']
            }


            for baseline_name, criterion in zip(['10_random', '10_most_violated'], ['random', 'most_violated_cycle']):
                if baseline_name in baseline.keys():
                    continue

                stats = {}
                sepa_hparams['criterion'] = criterion  # todo most_violated_cycle
                for scip_seed in dataset_config['scip_seed']:
                    model, x, y = maxcut_mccormic_model(G, use_general_cuts=False)
                    sepa = MccormickCycleSeparator(G=G, x=x, y=y, hparams=sepa_hparams)
                    model.includeSepa(
                        sepa, "MLCycles",
                        "Generate cycle inequalities for MaxCut using McCormic variables exchange",
                        priority=1000000,
                        freq=1
                    )
                    model.setRealParam('limits/time', dataset_config['time_limit_sec'])
                    model.setLongintParam('limits/nodes', 1)  # solve only at the root node
                    # rootonly_model.setIntParam('separating/maxstallroundsroot', -1)  # add cuts forever

                    # set up randomization
                    model.setBoolParam('randomization/permutevars', True)
                    model.setIntParam('randomization/permutationseed', scip_seed)
                    model.setIntParam('randomization/randomseedshift', scip_seed)
                    model.hideOutput(quiet=quiet)
                    model.optimize()
                    sepa.finish_experiment()
                    assert sepa.stats['lp_iterations'][-1] <= dataset_config['lp_iterations_limit']
                    stats[scip_seed] = sepa.stats

                baseline[baseline_name] = stats

            with open(filepath, 'wb') as f:
                pickle.dump((G, baseline), f)
                print('saved instance to ', filepath)

            if not quiet:
                print('saved graph to ', filepath)


if __name__ == '__main__':
    import argparse
    import yaml

    parser = argparse.ArgumentParser()
    parser.add_argument('--datadir', type=str, default='data/maxcut',
                        help='path to generate/read data')
    parser.add_argument('--configfile', type=str, default='configs/maxcut_data_config.yaml',
                        help='path to config file')
    # parser.add_argument('--workerid', type=int, default=0,
    #                     help='worker id')
    parser.add_argument('--nworkers', type=int, default=1,
                        help='total number of workers')
    parser.add_argument('--mp', type=str, default='none',
                        help='use ray [ray] or multiprocessing [mp] with nworkers')
    parser.add_argument('--quiet', action='store_true',
                        help='hide scip solving messages')
    args = parser.parse_args()

    # read data config
    with open(args.configfile) as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    config['datasets'].pop('trainset_20_30')

    # product the dataset configs and worker ids.
    configs = []
    # for dataset_name, dataset_config in config['datasets'].items():
    #     for k, v in vars(args).items():
    #         dataset_config[k] = v
    for workerid in range(args.nworkers):
        cfg = deepcopy(vars(args))
        cfg['datasets'] = config['datasets']
        cfg['workerid'] = workerid
        configs.append(cfg)

    if args.mp == 'mp':
        from multiprocessing import Pool
        with Pool() as p:
            res = p.map_async(solve_graphs, configs)
            res.wait()
            print(f'multiprocessing finished {"successfully" if res.successful() else "with errors"}')

    elif args.mp == 'ray':
        # from ray.tune import track
        # track.init(experiment_dir=args.datadir)
        tune_configs = tune.grid_search(configs)
        analysis = tune.run(solve_graphs,
                            config=tune_configs,
                            resources_per_trial={'cpu': 1, 'gpu': 0},
                            local_dir=args.datadir,
                            trial_name_creator=None,
                            max_failures=1  # TODO learn how to recover from checkpoints
                            )
    else:
        # process sequentially without threading
        for cfg in configs:
            solve_graphs(cfg)
    print('finished')

